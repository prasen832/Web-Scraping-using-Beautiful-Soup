{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ef315d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alasp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe55024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Input file\n",
    "df_Input = pd.read_excel('Input.xlsx')\n",
    "\n",
    "# Make a Directory to store the extracted text files\n",
    "article_dir = 'All_Articles'\n",
    "os.makedirs(article_dir, exist_ok=True)\n",
    "\n",
    "# Function to extract and save the articles\n",
    "\n",
    "def Extract_Article(url, url_id):\n",
    "   \n",
    "    # Send an HTTP request to the URL and get the webpage content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        \n",
    "        # Extract th title\n",
    "        title = soup.title.text if soup.title else \"Title not found\"\n",
    "        \n",
    "        # Extract main article\n",
    "        article = soup.article.p.parent\n",
    "         \n",
    "        \n",
    "        if article:\n",
    "        # Remove footer\n",
    "            for footer in article.find_all(class_=\"wp-block-preformatted\"):\n",
    "                footer.extract()\n",
    "\n",
    "            # Extract the text from article\n",
    "            main_article = article.get_text(' ',strip=True)\n",
    "            \n",
    "    \n",
    "            # Create a text file with the url_id as the file name\n",
    "            output_file = os.path.join(article_dir, f'{url_id}.txt') \n",
    "        \n",
    "            # Save the extraxted Title and Text in the text file \n",
    "            with open(output_file, 'w', encoding='utf-8') as file:\n",
    "                file.write(title + '.\\n\\n')\n",
    "                file.write(main_article)\n",
    "\n",
    "            print(f'Saved: {output_file}') # Print if article extracted and saved succesfully\n",
    "\n",
    "        else:\n",
    "            print('Main article not found on the page.')\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        print('Failed to retrieve the web page. Status code:', response.status_code)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0333e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: All_Articles\\123.0.txt\n",
      "Saved: All_Articles\\321.0.txt\n",
      "Saved: All_Articles\\2345.0.txt\n",
      "Saved: All_Articles\\4321.0.txt\n",
      "Saved: All_Articles\\432.0.txt\n",
      "Saved: All_Articles\\2893.8.txt\n",
      "Saved: All_Articles\\3355.6.txt\n",
      "Saved: All_Articles\\3817.4.txt\n",
      "Saved: All_Articles\\4279.2.txt\n",
      "Saved: All_Articles\\4741.0.txt\n",
      "Saved: All_Articles\\5202.8.txt\n",
      "Saved: All_Articles\\5664.6.txt\n",
      "Saved: All_Articles\\6126.4.txt\n",
      "Saved: All_Articles\\6588.2.txt\n",
      "Saved: All_Articles\\7050.0.txt\n",
      "Saved: All_Articles\\7511.8.txt\n",
      "Saved: All_Articles\\7973.6.txt\n",
      "Saved: All_Articles\\8435.4.txt\n",
      "Saved: All_Articles\\8897.2.txt\n",
      "Saved: All_Articles\\9359.0.txt\n",
      "Saved: All_Articles\\9820.8.txt\n",
      "Saved: All_Articles\\10282.6.txt\n",
      "Saved: All_Articles\\10744.4.txt\n",
      "Saved: All_Articles\\11206.2.txt\n",
      "Failed to retrieve the web page. Status code: 404\n",
      "Saved: All_Articles\\12129.8.txt\n",
      "Saved: All_Articles\\12591.6.txt\n",
      "Saved: All_Articles\\13053.4.txt\n",
      "Saved: All_Articles\\13515.2.txt\n",
      "Saved: All_Articles\\13977.0.txt\n",
      "Saved: All_Articles\\14438.8.txt\n",
      "Saved: All_Articles\\14900.6.txt\n",
      "Saved: All_Articles\\15362.4.txt\n",
      "Saved: All_Articles\\15824.2.txt\n",
      "Saved: All_Articles\\16286.0.txt\n",
      "Saved: All_Articles\\16747.8.txt\n",
      "Saved: All_Articles\\17209.6.txt\n",
      "Failed to retrieve the web page. Status code: 404\n",
      "Saved: All_Articles\\18133.2.txt\n",
      "Saved: All_Articles\\18595.0.txt\n",
      "Saved: All_Articles\\19056.8.txt\n",
      "Saved: All_Articles\\19518.6.txt\n",
      "Saved: All_Articles\\19980.4.txt\n",
      "Saved: All_Articles\\20442.2.txt\n",
      "Saved: All_Articles\\20904.0.txt\n",
      "Saved: All_Articles\\21365.8.txt\n",
      "Saved: All_Articles\\21827.6.txt\n",
      "Saved: All_Articles\\22289.4.txt\n",
      "Saved: All_Articles\\22751.2.txt\n",
      "Saved: All_Articles\\23213.0.txt\n",
      "Saved: All_Articles\\23674.8.txt\n",
      "Saved: All_Articles\\24136.6.txt\n",
      "Saved: All_Articles\\24598.4.txt\n",
      "Saved: All_Articles\\25060.2.txt\n",
      "Saved: All_Articles\\25522.0.txt\n",
      "Saved: All_Articles\\25983.8.txt\n",
      "Saved: All_Articles\\26445.6.txt\n",
      "Saved: All_Articles\\26907.4.txt\n",
      "Saved: All_Articles\\27369.2.txt\n",
      "Saved: All_Articles\\27831.0.txt\n",
      "Saved: All_Articles\\28292.8.txt\n",
      "Saved: All_Articles\\28754.6.txt\n",
      "Saved: All_Articles\\29216.4.txt\n",
      "Saved: All_Articles\\29678.2.txt\n",
      "Saved: All_Articles\\30140.0.txt\n",
      "Saved: All_Articles\\30601.8.txt\n",
      "Saved: All_Articles\\31063.6.txt\n",
      "Saved: All_Articles\\31525.4.txt\n",
      "Saved: All_Articles\\31987.2.txt\n",
      "Saved: All_Articles\\32449.0.txt\n",
      "Saved: All_Articles\\32910.8.txt\n",
      "Saved: All_Articles\\33372.6.txt\n",
      "Saved: All_Articles\\33834.4.txt\n",
      "Saved: All_Articles\\34296.2.txt\n",
      "Saved: All_Articles\\34758.0.txt\n",
      "Saved: All_Articles\\35219.8.txt\n",
      "Saved: All_Articles\\35681.6.txt\n",
      "Saved: All_Articles\\36143.4.txt\n",
      "Saved: All_Articles\\36605.2.txt\n",
      "Saved: All_Articles\\37067.0.txt\n",
      "Saved: All_Articles\\37528.8.txt\n",
      "Saved: All_Articles\\37990.6.txt\n",
      "Saved: All_Articles\\38452.4.txt\n",
      "Saved: All_Articles\\38914.2.txt\n",
      "Saved: All_Articles\\39376.0.txt\n",
      "Saved: All_Articles\\39837.8.txt\n",
      "Saved: All_Articles\\40299.6.txt\n",
      "Saved: All_Articles\\40761.4.txt\n",
      "Saved: All_Articles\\41223.2.txt\n",
      "Saved: All_Articles\\41685.0.txt\n",
      "Saved: All_Articles\\42146.8.txt\n",
      "Saved: All_Articles\\42608.6.txt\n",
      "Saved: All_Articles\\43070.4.txt\n",
      "Saved: All_Articles\\43532.2.txt\n",
      "Saved: All_Articles\\43994.0.txt\n",
      "Saved: All_Articles\\44455.8.txt\n",
      "Saved: All_Articles\\44917.6.txt\n",
      "Saved: All_Articles\\45379.4.txt\n",
      "Saved: All_Articles\\45841.2.txt\n",
      "Saved: All_Articles\\46303.0.txt\n",
      "Saved: All_Articles\\46764.8.txt\n",
      "Saved: All_Articles\\47226.6.txt\n",
      "Saved: All_Articles\\47688.4.txt\n",
      "Saved: All_Articles\\48150.2.txt\n",
      "Saved: All_Articles\\48612.0.txt\n",
      "Saved: All_Articles\\49073.8.txt\n",
      "Saved: All_Articles\\49535.6.txt\n",
      "Saved: All_Articles\\49997.4.txt\n",
      "Saved: All_Articles\\50459.2.txt\n",
      "Saved: All_Articles\\50921.0.txt\n",
      "Saved: All_Articles\\51382.8.txt\n",
      "Saved: All_Articles\\51844.6.txt\n",
      "Saved: All_Articles\\52306.4.txt\n",
      "Saved: All_Articles\\52768.2.txt\n",
      "Extraction Completed.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each URL in INPUT file to extract all articles\n",
    "\n",
    "for index, row in df_Input.iterrows():\n",
    "    URL_ID = row['URL_ID']\n",
    "    URL = row['URL']\n",
    "    Extract_Article(URL, URL_ID)\n",
    "\n",
    "print('Extraction Completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d8ad8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the stop word files in a List \n",
    "\n",
    "all_files = os.listdir('StopWords')\n",
    "\n",
    "#[file for file in all_files if file.endswith('.txt')]\n",
    "\n",
    "stop_word_files = [file for file in all_files if re.search(r'StopWords_\\w+.txt', file)]\n",
    "\n",
    "# Collect all stop words from all stop word Text files in a set.\n",
    "\n",
    "# Some text files contains extra info after pipe(|) operator which are not stop words.\n",
    "\n",
    "# So, collect the words before pipe(|) operator only.\n",
    "\n",
    "stop_words = set()\n",
    "\n",
    "for stop_file in stop_word_files:\n",
    "    with open('StopWords/' + stop_file, 'r') as file:\n",
    "        for line in file:\n",
    "            word = line.split('|')[0].strip()  # \n",
    "            stop_words.add(word)\n",
    "        \n",
    "stop_words_upper = {word.upper() for word in stop_words}\n",
    "\n",
    "\n",
    "# Extract and collect all positive words ina set.\n",
    "\n",
    "positive_words = set()\n",
    "with open('MasterDictionary/positive-words.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        word = line.strip()\n",
    "        if word:                       # Check if line is not empty\n",
    "            positive_words.add(word)\n",
    "\n",
    "# Extract and collect all negative words in a set.\n",
    "\n",
    "negative_words = set()\n",
    "with open('MasterDictionary/negative-words.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        word = line.strip()\n",
    "        if word:                       # Check if line is not empty\n",
    "            negative_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9ff903c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to all assignment operations\n",
    "\n",
    "def main(file, df):\n",
    "    \n",
    "    # Read the text file\n",
    "    with open('All_Articles/' + file, 'r', encoding='utf-8') as file:\n",
    "        \n",
    "        main_text = file.read()\n",
    "        \n",
    "        ID = re.findall(r'(\\d+.\\d*)(?:.txt)', str(file)) # Extract ID from file name\n",
    "        \n",
    "        ID = float(ID[0])  # Convert ID into float\n",
    "\n",
    "        words = nltk.word_tokenize(main_text)  # Tokenize the text\n",
    "\n",
    "        cleaned_text = []\n",
    "        \n",
    "        # Remove stop words \n",
    "        for word in words:\n",
    "            if word.upper() not in stop_words_upper:\n",
    "                cleaned_text.append(word)\n",
    "\n",
    "        # Sentimental Analysis\n",
    "\n",
    "        positive_score = 0\n",
    "        negative_score = 0\n",
    "        \n",
    "        # Get total count of positive and negative words in the article\n",
    "        for word in cleaned_text:\n",
    "            if word in positive_words:\n",
    "                positive_score = positive_score + 1\n",
    "            elif word in negative_words:\n",
    "                negative_score = negative_score + 1\n",
    "        \n",
    "        # Polarity Score\n",
    "        polarity_score = (positive_score - negative_score)/ ((positive_score + negative_score) + 0.000001)\n",
    "\n",
    "        total_words = len(cleaned_text)\n",
    "        \n",
    "        # Subjectivity Score\n",
    "        subjectivity_score = (positive_score + negative_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "\n",
    "\n",
    "        # Analysis of Readability\n",
    "\n",
    "        # 1. Average Sentence Length \n",
    "        \n",
    "        def remove_punctuations(main_text):\n",
    "\n",
    "            words = nltk.word_tokenize(main_text) # Get all tokens from text\n",
    "\n",
    "            cleaned_words = []\n",
    "            \n",
    "            # Remove Punctuations\n",
    "            for word in words:\n",
    "                if word not in string.punctuation:\n",
    "                     cleaned_words.append(word)\n",
    "\n",
    "            return cleaned_words\n",
    "\n",
    "        words_without_punctuations = remove_punctuations(main_text)\n",
    "\n",
    "        sentences = nltk.sent_tokenize(main_text)\n",
    "\n",
    "        total_sentences = len(sentences)\n",
    "\n",
    "        total_words_without_punctuations = len(words_without_punctuations)\n",
    "\n",
    "        average_sentence_length = total_words_without_punctuations / total_sentences\n",
    "\n",
    "\n",
    "        # 2. Percentage of Complex words \n",
    "\n",
    "        # Function to count syllables in the text\n",
    "        def syllable_count(word):\n",
    "\n",
    "            vowels = 'aeiou'\n",
    "\n",
    "            count = 0\n",
    "\n",
    "            for char in word:\n",
    "                if char.lower() in vowels:\n",
    "                    count = count + 1\n",
    "\n",
    "\n",
    "            if word.lower().endswith(\"es\") or word.lower().endswith(\"ed\"):\n",
    "                count -= 1\n",
    "\n",
    "            return count\n",
    "\n",
    "        # Function to count total complex words in the text\n",
    "        def complex_word_count(main_text):\n",
    "\n",
    "            words_C = nltk.word_tokenize(main_text)\n",
    "\n",
    "            total_complex_words = 0\n",
    "\n",
    "            for word in words_C:\n",
    "                if syllable_count(word) > 2:\n",
    "                    total_complex_words = total_complex_words + 1\n",
    "\n",
    "            return total_complex_words\n",
    "\n",
    "        total_complex_words = complex_word_count(main_text)\n",
    "\n",
    "        percentage_of_complex_words = (total_complex_words / total_words_without_punctuations)* 100\n",
    "\n",
    "        # 3. Fog Index\n",
    "\n",
    "        Fog_Index = 0.4 * (average_sentence_length + percentage_of_complex_words)\n",
    "\n",
    "        # Average Number of Words Per Sentence\n",
    "\n",
    "        average_sentence_length\n",
    "\n",
    "        # Complex Word Count\n",
    "\n",
    "        total_complex_words\n",
    "\n",
    "        # Word Count\n",
    "        \n",
    "        # To get word count remove NLTK stop words and punctuatins\n",
    "        def cleaned_words(main_text):\n",
    "\n",
    "            words = nltk.word_tokenize(main_text)\n",
    "\n",
    "            nltk_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "            cleaned_words = []\n",
    "\n",
    "            for word in words:\n",
    "                if word.lower() not in nltk_stop_words and word not in string.punctuation:\n",
    "                     cleaned_words.append(word)\n",
    "\n",
    "            return len(cleaned_words)\n",
    "\n",
    "        word_count = cleaned_words(main_text)   \n",
    "\n",
    "        # Syllable Count Per Word\n",
    "\n",
    "        total_syllables = 0\n",
    "        \n",
    "        # Call syllable_count function to get total syllable count per word\n",
    "        for word in words_without_punctuations:\n",
    "            count = syllable_count(word)\n",
    "            total_syllables = total_syllables + count\n",
    "\n",
    "        syllable_count_per_word = total_syllables / total_words_without_punctuations\n",
    "\n",
    "        # Personal Pronouns\n",
    "        \n",
    "        # Function to count personal pronouns using REGEX\n",
    "        def count_personal_pronouns(text):\n",
    "\n",
    "            pattern = r'\\s(I|we|my|ours|us)\\s'\n",
    "\n",
    "            matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
    "\n",
    "            count = 0\n",
    "\n",
    "            for match in matches:\n",
    "                if match == 'US':\n",
    "                    continue  # Skip the \"US\" match\n",
    "                count = count +  1\n",
    "\n",
    "            return count\n",
    "\n",
    "        total_personal_pronouns = count_personal_pronouns(main_text)\n",
    "\n",
    "        # Average Word Length\n",
    "\n",
    "        total_char = 0\n",
    "\n",
    "        for word in words_without_punctuations:\n",
    "            char = len(word)\n",
    "            total_char = total_char + char\n",
    "\n",
    "        average_word_length = total_char/len(words_without_punctuations)  \n",
    "\n",
    "        # Read the Output Data Structure file\n",
    "        #df = pd.read_excel('Output.xlsx')\n",
    "        \n",
    "        # Update the Output Data Structure file with our results\n",
    "        \n",
    "        df.loc[df['URL_ID'] == ID, 'POSITIVE SCORE'] = positive_score\n",
    "        df.loc[df['URL_ID'] == ID, 'NEGATIVE SCORE'] = negative_score\n",
    "        df.loc[df['URL_ID'] == ID, 'POLARITY SCORE'] = polarity_score\n",
    "        df.loc[df['URL_ID'] == ID, 'SUBJECTIVITY SCORE'] = subjectivity_score\n",
    "        df.loc[df['URL_ID'] == ID, 'AVG SENTENCE LENGTH'] = average_sentence_length\n",
    "        df.loc[df['URL_ID'] == ID, 'PERCENTAGE OF COMPLEX WORDS'] = percentage_of_complex_words\n",
    "        df.loc[df['URL_ID'] == ID, 'FOG INDEX'] = Fog_Index\n",
    "        df.loc[df['URL_ID'] == ID, 'AVG NUMBER OF WORDS PER SENTENCE'] = average_sentence_length\n",
    "        df.loc[df['URL_ID'] == ID, 'COMPLEX WORD COUNT'] = total_complex_words\n",
    "        df.loc[df['URL_ID'] == ID, 'WORD COUNT'] = word_count\n",
    "        df.loc[df['URL_ID'] == ID, 'SYLLABLE PER WORD'] = syllable_count_per_word\n",
    "        df.loc[df['URL_ID'] == ID, 'PERSONAL PRONOUNS'] = total_personal_pronouns\n",
    "        df.loc[df['URL_ID'] == ID, 'AVG WORD LENGTH'] = average_word_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2f838be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access this Directory \n",
    "all_docs = os.listdir('All_Articles')\n",
    "\n",
    "# Collect all article text files in a list\n",
    "docs = []\n",
    "for file in all_docs:\n",
    "    if file.endswith('.txt'):\n",
    "        docs.append(file)\n",
    "\n",
    "# Read the Output Data Structure file\n",
    "df = pd.read_excel('Output Data Structure.xlsx')\n",
    "\n",
    "# Iterate over each article for analysis\n",
    "for file in docs:\n",
    "    main(file, df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27edca51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Completed\n"
     ]
    }
   ],
   "source": [
    "# 2 URLs are not working so replace their columns with -1.\n",
    "\n",
    "df.fillna(-1, inplace=True)\n",
    "\n",
    "# This Excel file must be closed before updating it.\n",
    "df.to_excel('Output Data Structure.xlsx', index=False)    \n",
    "    \n",
    "print('Analysis Completed')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e9d6f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
